{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install --user Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "def conv_bn_relu(input_tensor, ch, kernel, padding=\"same\", strides=1, weight_decay=5e-4):\n",
    "    x = layers.Conv2D(ch, kernel, padding=padding, strides=strides,\n",
    "                      kernel_regularizer=keras.regularizers.l2(weight_decay))(input_tensor)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    return layers.Activation(\"relu\")(x)\n",
    "\n",
    "def stem_block(input_tensor):\n",
    "    x = conv_bn_relu(input_tensor, 32, 3, strides=2)\n",
    "    branch1 = conv_bn_relu(x, 16, 1)\n",
    "    branch1 = conv_bn_relu(branch1, 32, 3, strides=2)\n",
    "    branch2 = layers.MaxPool2D(2)(x)\n",
    "    x = layers.Concatenate()([branch1, branch2])\n",
    "    return conv_bn_relu(x, 32, 1)\n",
    "\n",
    "def dense_block(input_tensor, num_layers, growth_rate, bottleneck_width):\n",
    "    x = input_tensor\n",
    "    growth_rate = int(growth_rate / 2)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        inter_channel = int(growth_rate*bottleneck_width/4) * 4\n",
    "        branch1 = conv_bn_relu(x, inter_channel, 1)\n",
    "        branch1 = conv_bn_relu(branch1, growth_rate, 3)\n",
    "\n",
    "        branch2 = conv_bn_relu(x, inter_channel, 1)\n",
    "        branch2 = conv_bn_relu(branch2, growth_rate, 3)\n",
    "        branch2 = conv_bn_relu(branch2, growth_rate, 3)\n",
    "        x = layers.Concatenate()([x, branch1, branch2])\n",
    "    return x\n",
    "\n",
    "def transition_layer(input_tensor, k, use_pooling=True):\n",
    "    x = conv_bn_relu(input_tensor, k, 1)\n",
    "    if use_pooling:\n",
    "        return layers.AveragePooling2D(2)(x)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def PeleeNet(input_shape=(224,224,3), use_stem_block=True, n_classes=1000):\n",
    "    n_dense_layers = [3,4,8,6]\n",
    "    bottleneck_width = [1,2,4,4]\n",
    "    out_layers = [128,256,512,704]\n",
    "    growth_rate = 32\n",
    "\n",
    "    input = layers.Input(input_shape)\n",
    "    x = stem_block(input) if use_stem_block else input\n",
    "    for i in range(4):\n",
    "        x = dense_block(x, n_dense_layers[i], growth_rate, bottleneck_width[i])\n",
    "        use_pooling = i < 3\n",
    "        x = transition_layer(x, out_layers[i], use_pooling=use_pooling)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(n_classes, activation=\"softmax\")(x)\n",
    "    return keras.models.Model(input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
      "169009152/169001437 [==============================] - 7s 0us/step\n",
      "WARNING:tensorflow:From <ipython-input-2-23d896e38582>:88: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 390 steps, validate for 10 steps\n",
      "Epoch 1/120\n",
      "389/390 [============================>.] - ETA: 0s - loss: 6.0712 - acc: 0.0687\n",
      "Epoch 00001: val_loss improved from inf to 4.78452, saving model to ./peleenet-cifar100-batch_size-128/1/saved_model\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ./peleenet-cifar100-batch_size-128/1/saved_model/assets\n",
      "390/390 [==============================] - 190s 488ms/step - loss: 6.0669 - acc: 0.0687 - val_loss: 4.7845 - val_acc: 0.0459\n",
      "Epoch 2/120\n",
      "389/390 [============================>.] - ETA: 0s - loss: 3.8172 - acc: 0.1584\n",
      "Epoch 00002: val_loss did not improve from 4.78452\n",
      "390/390 [==============================] - 134s 344ms/step - loss: 3.8172 - acc: 0.1584 - val_loss: 8.4724 - val_acc: 0.0382\n",
      "Epoch 3/120\n",
      "389/390 [============================>.] - ETA: 0s - loss: 3.4398 - acc: 0.2402\n",
      "Epoch 00003: val_loss did not improve from 4.78452\n",
      "390/390 [==============================] - 138s 354ms/step - loss: 3.4394 - acc: 0.2403 - val_loss: 6.5163 - val_acc: 0.0720\n",
      "Epoch 4/120\n",
      "389/390 [============================>.] - ETA: 0s - loss: 3.2206 - acc: 0.3031\n",
      "Epoch 00004: val_loss improved from 4.78452 to 3.82731, saving model to ./peleenet-cifar100-batch_size-128/1/saved_model\n",
      "INFO:tensorflow:Assets written to: ./peleenet-cifar100-batch_size-128/1/saved_model/assets\n",
      "390/390 [==============================] - 178s 457ms/step - loss: 3.2204 - acc: 0.3031 - val_loss: 3.8273 - val_acc: 0.2237\n",
      "Epoch 5/120\n",
      "389/390 [============================>.] - ETA: 0s - loss: 3.1155 - acc: 0.3435\n",
      "Epoch 00005: val_loss did not improve from 3.82731\n",
      "390/390 [==============================] - 136s 349ms/step - loss: 3.1159 - acc: 0.3436 - val_loss: 3.9122 - val_acc: 0.2182\n",
      "Epoch 6/120\n",
      "389/390 [============================>.] - ETA: 0s - loss: 3.0491 - acc: 0.3649\n",
      "Epoch 00006: val_loss did not improve from 3.82731\n",
      "390/390 [==============================] - 136s 349ms/step - loss: 3.0493 - acc: 0.3649 - val_loss: 4.2385 - val_acc: 0.1993\n",
      "Epoch 7/120\n",
      "389/390 [============================>.] - ETA: 0s - loss: 2.9955 - acc: 0.3836\n",
      "Epoch 00007: val_loss improved from 3.82731 to 3.63929, saving model to ./peleenet-cifar100-batch_size-128/1/saved_model\n",
      "INFO:tensorflow:Assets written to: ./peleenet-cifar100-batch_size-128/1/saved_model/assets\n",
      "390/390 [==============================] - 180s 461ms/step - loss: 2.9953 - acc: 0.3837 - val_loss: 3.6393 - val_acc: 0.2632\n",
      "Epoch 8/120\n",
      "389/390 [============================>.] - ETA: 0s - loss: 2.9541 - acc: 0.4011\n",
      "Epoch 00008: val_loss improved from 3.63929 to 3.51337, saving model to ./peleenet-cifar100-batch_size-128/1/saved_model\n",
      "INFO:tensorflow:Assets written to: ./peleenet-cifar100-batch_size-128/1/saved_model/assets\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 2.9538 - acc: 0.4011 - val_loss: 3.5134 - val_acc: 0.2921\n",
      "Epoch 9/120\n",
      "389/390 [============================>.] - ETA: 0s - loss: 2.9114 - acc: 0.4165\n",
      "Epoch 00009: val_loss did not improve from 3.51337\n",
      "390/390 [==============================] - 134s 343ms/step - loss: 2.9117 - acc: 0.4163 - val_loss: 4.2917 - val_acc: 0.1701\n",
      "Epoch 10/120\n",
      "389/390 [============================>.] - ETA: 0s - loss: 2.8815 - acc: 0.4320\n",
      "Epoch 00010: val_loss did not improve from 3.51337\n",
      "390/390 [==============================] - 137s 350ms/step - loss: 2.8815 - acc: 0.4321 - val_loss: 3.7183 - val_acc: 0.2667\n",
      "Epoch 11/120\n",
      "389/390 [============================>.] - ETA: 0s - loss: 2.8477 - acc: 0.4460\n",
      "Epoch 00011: val_loss did not improve from 3.51337\n",
      "390/390 [==============================] - 135s 346ms/step - loss: 2.8478 - acc: 0.4460 - val_loss: 4.7693 - val_acc: 0.1868\n",
      "Epoch 12/120\n",
      "389/390 [============================>.] - ETA: 0s - loss: 2.8237 - acc: 0.4555\n",
      "Epoch 00012: val_loss did not improve from 3.51337\n",
      "390/390 [==============================] - 136s 348ms/step - loss: 2.8238 - acc: 0.4555 - val_loss: 3.9208 - val_acc: 0.2670\n",
      "Epoch 13/120\n",
      "389/390 [============================>.] - ETA: 0s - loss: 2.7936 - acc: 0.4648\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.20000000298023224.\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 3.51337\n",
      "390/390 [==============================] - 135s 346ms/step - loss: 2.7932 - acc: 0.4649 - val_loss: 4.5032 - val_acc: 0.2243\n",
      "Epoch 14/120\n",
      "140/390 [=========>....................] - ETA: 1:11 - loss: 2.4296 - acc: 0.5392"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "EPOCHS = 120\n",
    "BATCH_SIZE = 128\n",
    "MODEL_VERSION = 1\n",
    "MODEL_NAME = f'peleenet-cifar100-batch_size-{BATCH_SIZE}'\n",
    "LOGS = 'logs'\n",
    "\n",
    "model_directory = os.path.join(f'./{MODEL_NAME}')\n",
    "\n",
    "if os.path.isdir(model_directory):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(model_directory)\n",
    "    os.mkdir(os.path.join(model_directory, str(MODEL_VERSION)))\n",
    "    os.mkdir(os.path.join(os.path.join(model_directory), str(MODEL_VERSION), LOGS))\n",
    "\n",
    "\n",
    "def generator(X, y, batch_size, use_augmentation, shuffle, scale):\n",
    "    if use_augmentation:\n",
    "        base_gen = keras.preprocessing.image.ImageDataGenerator(\n",
    "            horizontal_flip=True,\n",
    "            width_shift_range=4.0/32.0,\n",
    "            height_shift_range=4.0/32.0)\n",
    "    else:\n",
    "        base_gen = keras.preprocessing.image.ImageDataGenerator()\n",
    "    for X_base, y_base in base_gen.flow(X, y, batch_size=batch_size, shuffle=shuffle):\n",
    "        if scale != 1:\n",
    "            X_batch = np.zeros((X_base.shape[0], X_base.shape[1]*scale,\n",
    "                                X_base.shape[2]*scale, X_base.shape[3]), np.float32)\n",
    "            for i in range(X_base.shape[0]):\n",
    "                with Image.fromarray(X_base[i].astype(np.uint8)) as img:\n",
    "                    img = img.resize((X_base.shape[1]*scale, X_base.shape[2]*scale), Image.LANCZOS)\n",
    "                    X_batch[i] = np.asarray(img, np.float32) / 255.0\n",
    "        else:\n",
    "            X_batch = X_base / 255.0\n",
    "        yield X_batch, y_base\n",
    "\n",
    "# def lr_scheduler(epoch):\n",
    "#     x = 0.4\n",
    "#     if epoch >= 20: x /= 5.0\n",
    "#     if epoch >= 50: x /= 5.0\n",
    "#     if epoch >= 80: x /= 5.0\n",
    "#     if epoch >= 100: x /= 5.0\n",
    "#     return x\n",
    "\n",
    "def train(use_augmentation, use_stem_block):\n",
    "    #tf.compat.v1.logging.set_verbosity(tf.logging.FATAL)\n",
    "    (X_train, y_train), (X_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "    y_train = keras.utils.to_categorical(y_train)\n",
    "    y_test = keras.utils.to_categorical(y_test)\n",
    "    \n",
    "   \n",
    "    # generator\n",
    "    scale = 7 if use_stem_block else 1\n",
    "    train_gen = generator(X_train, y_train, batch_size=BATCH_SIZE,\n",
    "                          use_augmentation=use_augmentation, shuffle=True, scale=scale)\n",
    "    test_gen = generator(X_test, y_test, batch_size=2000,\n",
    "                         use_augmentation=False, shuffle=False, scale=scale)\n",
    "        \n",
    "    # network\n",
    "    input_shape = (224,224,3) if use_stem_block else (32,32,3)\n",
    "    model = PeleeNet(input_shape=input_shape, use_stem_block=use_stem_block, n_classes=100)\n",
    "    model.compile(keras.optimizers.SGD(0.4, 0.9), \"categorical_crossentropy\", [\"acc\"])\n",
    "    \n",
    "\n",
    "    #scheduler = keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
    "    hist = keras.callbacks.History()\n",
    "    tensorboard = keras.callbacks.TensorBoard(log_dir=f'./{MODEL_NAME}/{MODEL_VERSION}/{LOGS}', histogram_freq=20, write_graph=True, \n",
    "                                              write_images=False, update_freq='batch', profile_batch=2,\n",
    "                                             embeddings_freq=20, embeddings_metadata=None)\n",
    "    \n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(filepath=f'./{MODEL_NAME}/{MODEL_VERSION}/saved_model', monitor='val_loss', verbose=1, \n",
    "                                                 save_best_only=True, save_weights_only=False, mode='auto', \n",
    "                                                 save_freq='epoch')\n",
    "    \n",
    "    lr_plateau = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, \n",
    "                                                   verbose=1, mode='auto', cooldown=0, min_lr=0.0001)\n",
    "\n",
    "    model.fit_generator(train_gen, steps_per_epoch=X_train.shape[0]//BATCH_SIZE,\n",
    "                        validation_data=test_gen, validation_steps=X_test.shape[0]//1000,\n",
    "                        callbacks=[lr_plateau, hist, tensorboard, checkpoint], epochs=EPOCHS, max_queue_size=1)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    train(True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
